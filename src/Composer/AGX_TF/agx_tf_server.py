"""
Author: Aimilios Leftheriotis
Affiliations: Microlab@NTUA, VLSILab@UPatras

This module defines the AgxTfServer class, which inherits from the BaseExperimentServer class defined in the experiment_server module.
The AgxTfServer class represents an AGX-based implementation of the experiment server using native TensorFlow.

Overview:
- The AgxTfServer class is responsible for initializing the TensorFlow environment, loading the model, and executing inference experiments.
- It includes methods for single and multiple inference runs, warm-up routines, and platform-specific preprocessing and postprocessing.

Classes:
- AgxTfServer: Inherits from BaseExperimentServer and implements AGX_TF-specific initialization and inference methods.

Methods:
- __init__(self, logger): Initializes the AgxTfServer instance, sets up the logger, initializes the kernel, and performs a warm-up run.
- init_kernel(self): Loads the TensorFlow model and configures execution settings.
- warm_up(self): Performs a warm-up run by passing a dummy input through the TensorFlow model.
- experiment_single(self, input, run_total=1): Executes a single experiment, taking a numpy array as input and returning a numpy array as output.
- experiment_multiple(self, dataset, run_total): Executes multiple experiments, taking a tf.data.Dataset as input and returning a numpy array as output.
- platform_preprocess(self, data): Placeholder for AI-framework/platform pair-specific preprocessing.
- platform_postprocess(self, data): Placeholder for AI-framework/platform pair-specific postprocessing.

DO NOT edit this file directly.
"""

import os
import time
import numpy as np
import tensorflow as tf

import experiment_server

class AgxTfServer(experiment_server.BaseExperimentServer):
    """
    AGX_TF-specific server implementation for running TensorFlow models.
    Inherits from BaseExperimentServer and implements AGX_TF-specific initialization and inference methods.
    """
    def __init__(self, logger):
        """Initialize the AgxTfServer instance, set up the logger, initialize the kernel, and perform a warm-up run."""
        super().__init__(logger)
        self.model = None
        self.init_kernel()
        self.warm_up()

    def init_kernel(self):
        """
        Initialize one-time AI-framework/platform pair-specific server operations.
        Loads the TensorFlow model and configures execution settings.
        """
        start = time.perf_counter()

        # Load the Keras model from the specified path
        self.model = tf.keras.models.load_model(filepath=self.server_configs['MODEL_PATH'])

        # Store input and output shapes in the server configurations        
        self.server_configs['input_shape'] = self.model.input_shape
        self.server_configs['output_shape'] = self.model.output_shape

        # Logging for debugging
        self.log(f"Input Shape: {self.server_configs['input_shape']}")
        self.log(f"Output Shape: {self.server_configs['output_shape']}")

        end = time.perf_counter()
        self.once_timings['init'] = end - start
        self.log(f"Initialize time: {self.once_timings['init'] * 1000:.2f} ms")
    
    def warm_up(self):
        """
        Run first-time AI-framework/platform pair-specific server operations.
        Warm-up the TensorFlow model by running a dummy input.
        """
        start = time.perf_counter()

        # Create a dummy input with zeros and set it as the input tensor
        x_dummy = np.zeros(shape=(self.server_configs['BATCH_SIZE'],) + self.server_configs['input_shape'][1:], dtype=np.float32)
        _ = self.model.predict(x=x_dummy, batch_size=self.server_configs['BATCH_SIZE'], verbose=0)

        end = time.perf_counter()
        self.once_timings['warm_up'] = end - start
        self.log(f"Warmup time: {self.once_timings['warm_up'] * 1000:.2f} ms")

    def experiment_single(self, input, run_total=1):
        """
        Execute the experiment for single input data.
        Works only in Latency Server Mode (self.server_configs['SERVER_MODE'] == 0).
        Takes a numpy array as input and returns a numpy array as output.
        """
        exp_output = self.model.predict(x=input, verbose=0)
        return exp_output

    def experiment_multiple(self, dataset, run_total):
        """
        Execute the experiment for multiple input data.
        Works only in Throughput Server Mode (self.server_configs['SERVER_MODE'] == 1).
        Takes a tf.data.Dataset as input and returns a numpy array as output.
        """
        output_list = self.model.predict(x=dataset, verbose=0)

        # Concatenate all individual outputs to form a single numpy array
        concat_start = time.perf_counter()
        exp_output = np.concatenate(output_list, axis=0)
        concat_end = time.perf_counter()

        self.log(f"Concat output time: {(concat_end - concat_start) * 1000:.2f} ms")
        return exp_output

    def platform_preprocess(self, data):
        """Preprocess the input, specific to the platform requirement, not the experiment ones. Used on create_and_preprocess as the last step."""
        return data

    def platform_postprocess(self, data):
        """Postprocess the output, specific to the platform requirement, not the experiment ones. Used on postprocess as the first step."""
        return data
